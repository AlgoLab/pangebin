{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../dataset\"\n",
    "test_id = 'ismb2025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_score_to_tuple(sample, score, input_type, mix):\n",
    "    cols = [\"sample\", \"species\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"]\n",
    "    out_tuple = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # out_tuple = pd.DataFrame(columns=cols)\n",
    "    # voglio leggere i file .txt del typo \"score\" (lab o bin) e del tipo \"input_type\" (ske, uni, pan)\n",
    "    # e metterli in un dataframe\n",
    "    # se input type == pan allora devo fare la \"media\" dei valori di ske e uni\n",
    "    # riguardo al tipo di bin: uni e ske hanno solo pred, mentre pan ha anche naive e graph-overlap\n",
    "    for filename in os.listdir(sample):\n",
    "        fields = filename.split(\".\")\n",
    "        if (not filename.endswith(\".txt\")): # salto i file non txt, non sono file di score\n",
    "            continue\n",
    "        _species = \"null\"\n",
    "        _sample = fields[0] #nome del sample\n",
    "        _type = fields[1] # tipo in input {ske, uni, pan}\n",
    "        _thr = fields[2] # threshold (default 1)\n",
    "        _model = fields[3] # modello {pbf, ml}, default pbf\n",
    "        assert(_model == \"pbf\")\n",
    "        _score = fields[-2] # score {lab, bin} labeling o binning\n",
    "        _ref = fields[-3] # reference {uni, ske}\n",
    "        _bins = fields[-4] # tipo di binning {pred, nve, ovl} pred= predizione del modello, nve= naive, ovl= graph-overlap\n",
    "        \n",
    "        if _type == input_type:\n",
    "            if _score == \"lab\" and score == \"lab\": # se il tipo di score del file Ã¨ del tipo richiesto\n",
    "                assert(filename.endswith(\".txt\"))\n",
    "                with open(sample+ \"/\" + filename) as file:\n",
    "                    lines = file.readlines()\n",
    "                    prec, rec, f1 = float(lines[-4].split(\"\\t\")[-1].strip()), float(lines[-3].split(\"\\t\")[-1].strip()), float(lines[-2].split(\"\\t\")[-1].strip())\n",
    "\n",
    "            elif _score == \"bin\" and score == \"bin\":\n",
    "                assert(filename.endswith(\".txt\"))\n",
    "                with open(sample+ \"/\" + filename) as file:\n",
    "                    lines = file.readlines()\n",
    "                    prec, rec, f1 = float(lines[-3].split(\"\\t\")[-1].strip()), float(lines[-2].split(\"\\t\")[-1].strip()), float(lines[-1].split(\"\\t\")[-1].strip())\n",
    "            else:\n",
    "                continue\n",
    "            out_tuple = pd.concat([pd.DataFrame([[_sample, _species, _model, _bins, f\"{_thr}.{_type}\", prec, rec, f1]], columns=cols), out_tuple if not out_tuple.empty else None], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "        # print(out_tuple)\n",
    "        if len(out_tuple) > 1:\n",
    "            out_tuple = out_tuple.groupby([\"sample\", \"model\", \"species\", \"bintype\", \"tool\"]).mean(numeric_only=True).reset_index()\n",
    "            # out_tuple.insert(loc=3, column=\"tool\", value = f\"{_thr}.{_type}\")\n",
    "            if (mix):\n",
    "                out_tuple = out_tuple.groupby([\"sample\", \"model\", \"species\", \"tool\"]).max(numeric_only=True).reset_index()\n",
    "                out_tuple.insert(loc=2, column=\"bintype\", value = f\"pred\")\n",
    "                # out_tuple.insert(loc=3, column=\"tool\", value = f\"{_thr}.{_type}\")\n",
    "            \n",
    "\n",
    "    return out_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gplas_to_tuple(filename, sample, species, type, score, zero):\n",
    "    _model = \"gplas\"\n",
    "    _species = species\n",
    "    _sample = sample\n",
    "    _type = type # uni, ske\n",
    "    _thr = 1\n",
    "    _score = score # lab, bin\n",
    "    _ref = type\n",
    "    _bins = \"pred\"\n",
    "    filename = filename # SAMNXXXXX-u.gplas.bin.txt\n",
    "    cols = [\"sample\", \"species\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"]\n",
    "    out_tuple = pd.DataFrame(columns=cols)\n",
    "    fields = filename.split(\".\")\n",
    "    if (not filename.endswith(\".txt\")): # salto i file non txt, non sono file di score\n",
    "        return None\n",
    "    if _score == \"lab\":\n",
    "        if zero:\n",
    "            prec, rec, f1 = 0, 0, 0\n",
    "        else:   \n",
    "            with open(filename) as file:\n",
    "                    lines = file.readlines()\n",
    "                    prec, rec, f1 = float(lines[-4].split(\"\\t\")[-1].strip()), float(lines[-3].split(\"\\t\")[-1].strip()), float(lines[-2].split(\"\\t\")[-1].strip())\n",
    "    elif _score == \"bin\":\n",
    "        if zero:\n",
    "            prec, rec, f1 = 0, 0, 0\n",
    "        else:\n",
    "            with open(filename) as file:\n",
    "                    lines = file.readlines()\n",
    "                    prec, rec, f1 = float(lines[-3].split(\"\\t\")[-1].strip()), float(lines[-2].split(\"\\t\")[-1].strip()), float(lines[-1].split(\"\\t\")[-1].strip())\n",
    "                    \n",
    "    else:\n",
    "        return None\n",
    "    out_tuple = pd.concat([pd.DataFrame([[_sample, _species, _model, _bins, f\"{_thr}.{_type}\", prec, rec, f1]], columns=cols), out_tuple if not out_tuple.empty else None], ignore_index=True)\n",
    "\n",
    "\n",
    "    assert len(out_tuple) == 1\n",
    "    return out_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all samples\n",
    "df_labeling = pd.DataFrame(columns=[\"sample\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"])\n",
    "df_labeling_mix = pd.DataFrame(columns=[\"sample\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"])\n",
    "df_binning = pd.DataFrame(columns=[\"sample\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"])\n",
    "df_binning_mix = pd.DataFrame(columns=[\"sample\", \"model\", \"bintype\", \"tool\", \"prec\", \"rec\", \"f1\"])\n",
    "for file in os.listdir(data_folder):\n",
    "    sample = data_folder + file\n",
    "    for input_type in [\"ske\", \"uni\", \"pan\"]:\n",
    "        df_labeling = pd.concat([read_score_to_tuple(sample, \"lab\", input_type, mix=False), df_labeling if not df_labeling.empty else None], ignore_index=True)\n",
    "        df_labeling_mix = pd.concat([read_score_to_tuple(sample, \"lab\", input_type, mix=True), df_labeling_mix if not df_labeling_mix.empty else None], ignore_index=True)\n",
    "\n",
    "        df_binning = pd.concat([read_score_to_tuple(sample, \"bin\", input_type, mix=False), df_binning if not df_binning.empty else None], ignore_index=True)\n",
    "        df_binning_mix = pd.concat([read_score_to_tuple(sample, \"bin\", input_type, mix=True), df_binning_mix if not df_binning_mix.empty else None], ignore_index=True)\n",
    "\n",
    "gplas_res = \"/home/sgro/gplas/results\"\n",
    "gplas_data = \"/data/proj/pangebin/data-gplas/\"\n",
    "# def read_gplas_to_tuple(filename, sample, type, score):\n",
    "\n",
    "for species in os.listdir(gplas_data):\n",
    "    speciesStr = species.casefold().split(\" \")[0]\n",
    "    for id in os.listdir(f\"{gplas_data}/{species}\"):\n",
    "        zero_lab_s = False\n",
    "        zero_lab_u = False\n",
    "        zero_bin_s = False\n",
    "        zero_bin_u = False\n",
    "\n",
    "        labeling_res_u = f\"{gplas_res}/{id}-u.gplas.lab.txt\"\n",
    "        labeling_res_s = f\"{gplas_res}/{id}-s.gplas.lab.txt\"\n",
    "\n",
    "        binning_res_u = f\"{gplas_res}/{id}-u.gplas.bin.txt\"\n",
    "        binning_res_s = f\"{gplas_res}/{id}-s.gplas.bin.txt\"\n",
    "        try:\n",
    "            assert os.path.exists(labeling_res_u)\n",
    "        except AssertionError:\n",
    "            zero_lab_u = True\n",
    "        try:\n",
    "            assert os.path.exists(labeling_res_s)\n",
    "        except AssertionError:\n",
    "            zero_lab_s = True\n",
    "        try:\n",
    "            assert os.path.exists(binning_res_u)\n",
    "        except AssertionError:\n",
    "            zero_bin_u = True\n",
    "        try:\n",
    "            assert os.path.exists(binning_res_s)\n",
    "        except AssertionError:\n",
    "            zero_bin_s = True\n",
    "\n",
    "        df_labeling = pd.concat([read_gplas_to_tuple(labeling_res_u, f\"{id}\", speciesStr, \"uni\", \"lab\", zero_lab_u), df_labeling if not df_labeling.empty else None], ignore_index=True)\n",
    "        df_labeling = pd.concat([read_gplas_to_tuple(labeling_res_s, f\"{id}\", speciesStr, \"ske\", \"lab\", zero_lab_s), df_labeling if not df_labeling.empty else None], ignore_index=True)\n",
    "\n",
    "        df_binning = pd.concat([read_gplas_to_tuple(binning_res_u, f\"{id}\", speciesStr,  \"uni\", \"bin\", zero_bin_u), df_binning if not df_binning.empty else None], ignore_index=True)\n",
    "        df_binning = pd.concat([read_gplas_to_tuple(binning_res_s, f\"{id}\", speciesStr, \"ske\", \"bin\", zero_bin_s), df_binning if not df_binning.empty else None], ignore_index=True)\n",
    "\n",
    "        \n",
    "\n",
    "df_labeling = df_labeling[df_labeling.bintype != \"nve\"]\n",
    "df_labeling = df_labeling[df_labeling.bintype != \"ovl\"]\n",
    "# taken a sample, replace species with null value with the species from the same sample id\n",
    "for index, row in df_labeling.iterrows():\n",
    "    if row[\"tool\"] == \"1.pan\":\n",
    "        row[\"model\"] == \"pan\"\n",
    "    if row[\"species\"] == \"null\":\n",
    "        sample = row[\"sample\"]\n",
    "        row[\"species\"] = df_labeling[df_labeling[\"sample\"] == sample].species.unique()[0]\n",
    "    df_labeling.loc[index] = row\n",
    "\n",
    "df_labeling = df_labeling.sort_values(by=[\"species\", \"model\", \"bintype\", \"tool\"])\n",
    "\n",
    "\n",
    "\n",
    "df_binning = df_binning[df_binning.bintype != \"nve\"]\n",
    "df_binning = df_binning[df_binning.bintype != \"ovl\"]\n",
    "for index, row in df_binning.iterrows():\n",
    "    if row[\"tool\"] == \"1.pan\":\n",
    "        row[\"model\"] == \"pan\"\n",
    "    if row[\"species\"] == \"null\":\n",
    "        sample = row[\"sample\"]\n",
    "        row[\"species\"] = df_binning[df_binning[\"sample\"] == sample].species.unique()[0]\n",
    "    df_binning.loc[index] = row\n",
    "\n",
    "df_binning = df_binning.sort_values(by=[\"species\", \"model\", \"bintype\", \"tool\"])\n",
    "\n",
    "\n",
    "\n",
    "df_labeling.to_csv(f\"results/{test_id}.labeling.csv\", sep='\\t', index=False)\n",
    "df_binning.to_csv(f\"results/{test_id}.binning.csv\", sep='\\t', index=False)\n",
    "\n",
    "df_labeling_mix.to_csv(f\"results/{test_id}.labeling.mix.csv\", sep='\\t', index=False)\n",
    "df_binning_mix.to_csv(f\"results/{test_id}.binning.mix.csv\", sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "{'SAMN16357205', 'SAMN16357459', 'SAMN16357501', 'SAMN16357460', 'SAMN16357462', 'SAMN01057614', 'SAMN16357502'}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Removing samples, acinetobacter and enterococcus because not in the plasmid database\n",
    "\n",
    "df_lab = df_labeling[df_labeling[\"species\"] != \"acinetobacter\"]\n",
    "df_lab = df_lab[df_lab[\"species\"] != \"enterococcus\"]\n",
    "\n",
    "df_bin = df_binning[df_binning[\"species\"] != \"acinetobacter\"]\n",
    "df_bin = df_bin[df_bin[\"species\"] != \"enterococcus\"]\n",
    "\n",
    "# extract ids that have precision, recall and f1 equal to 0 for 1.uni and 1.ske, i.e. solution was empty meaning that the plasmid database matching step provided non plasmids for sample.\n",
    "ids_u = df_lab.loc[(df_lab[\"prec\"] == 0) & (df_lab[\"rec\"] == 0) & (df_lab[\"model\"]=='pbf') & (df_lab[\"tool\"]=='1.uni'), \"sample\"].unique()\n",
    "ids_s = df_lab.loc[(df_lab[\"prec\"] == 0) & (df_lab[\"rec\"] == 0) & (df_lab[\"model\"]=='pbf') & (df_lab[\"tool\"]=='1.ske'), \"sample\"].unique()\n",
    "\n",
    "print(len(ids_u))\n",
    "print(len(ids_s))\n",
    "\n",
    "remove_set = set(ids_u).intersection(set(ids_s))\n",
    "print(remove_set)\n",
    "print(len(remove_set))\n",
    "# remove_set = remove_set - {'SAMN16357502'}\n",
    "\n",
    "df_lab_cut = df_lab[~df_lab[\"sample\"].isin(remove_set)]\n",
    "df_bin_cut = df_bin[~df_bin[\"sample\"].isin(remove_set)]\n",
    "\n",
    "df_lab_cut.to_csv(f\"results/{test_id}.labeling.cut.csv\", sep='\\t', index=False)\n",
    "df_bin_cut.to_csv(f\"results/{test_id}.binning.cut.csv\", sep='\\t', index=False)\n",
    "\n",
    "df_lab_rem = df_lab[df_lab[\"sample\"].isin(remove_set)]\n",
    "df_bin_rem = df_bin[df_bin[\"sample\"].isin(remove_set)]\n",
    "\n",
    "df_lab_rem.to_csv(f\"results/{test_id}.labeling.rem.csv\", sep='\\t', index=False)\n",
    "df_bin_rem.to_csv(f\"results/{test_id}.binning.rem.csv\", sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PLOT LABELING\n",
    "df_labeling = pd.read_csv(f\"results/{test_id}.labeling.cut.csv\", sep='\\t')\n",
    "df_binning = pd.read_csv(f\"results/{test_id}.binning.cut.csv\", sep='\\t')\n",
    "# PLOT GPLAS unicycler vs PBF unicyler\n",
    "len(df_labeling[\"sample\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species    \n",
       "klebsiella     40.0\n",
       "escherichia     8.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeling[[\"species\"]].value_counts()/5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.legend_handler import HandlerTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(type_):\n",
    "    assert type_ in [\"labeling\", \"binning\"]\n",
    "    global df_scores, df_pbf_unicycler, df_gplas_unicycler, df_pangebin, df_pbf_skesa, df_gplas_skesa, df_pbf_mean, df_gplas_mean\n",
    "    df_scores = pd.read_csv(f\"{test_id}.{type_}.cut.csv\", sep='\\t')\n",
    "    df_pbf_unicycler = df_scores[(df_scores[\"tool\"] == \"1.uni\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_unicycler = df_pbf_unicycler.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_gplas_unicycler = df_scores[(df_scores[\"tool\"] == \"1.uni\") & (df_scores[\"model\"] == \"gplas\")]\n",
    "    df_gplas_unicycler = df_gplas_unicycler.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_pangebin = df_scores[(df_scores[\"tool\"]==\"1.pan\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pangebin = df_pangebin.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_pbf_skesa= df_scores[(df_scores[\"tool\"] == \"1.ske\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_skesa = df_pbf_skesa.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_gplas_skesa = df_scores[(df_scores[\"tool\"] == \"1.ske\") & (df_scores[\"model\"] == \"gplas\")]\n",
    "    df_gplas_skesa = df_gplas_skesa.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_pbf_mean = df_scores[(df_scores[\"tool\"] != \"1.pan\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_mean = df_pbf_mean.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_gplas_mean = df_scores[(df_scores[\"tool\"] != \"1.pan\") & (df_scores[\"model\"] == \"gplas\")]\n",
    "    df_gplas_mean = df_gplas_mean.groupby([\"sample\"]).mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def boxplot(df_scores, scoretype):\n",
    "\n",
    "    df_pbf_unicycler = df_scores[(df_scores[\"tool\"] == \"1.uni\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_unicycler = df_pbf_unicycler.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_gplas_unicycler = df_scores[(df_scores[\"tool\"] == \"1.uni\") & (df_scores[\"model\"] == \"gplas\")]\n",
    "    df_gplas_unicycler = df_gplas_unicycler.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_pangebin = df_scores[(df_scores[\"tool\"]==\"1.pan\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pangebin = df_pangebin.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_pbf_skesa= df_scores[(df_scores[\"tool\"] == \"1.ske\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_skesa = df_pbf_skesa.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_gplas_skesa = df_scores[(df_scores[\"tool\"] == \"1.ske\") & (df_scores[\"model\"] == \"gplas\")]\n",
    "    df_gplas_skesa = df_gplas_skesa.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "    df_pbf_mean = df_scores[(df_scores[\"tool\"] != \"1.pan\") & (df_scores[\"model\"] == \"pbf\")]\n",
    "    df_pbf_mean = df_pbf_mean.groupby([\"sample\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "    header = [\"tool\", \"score\", \"score_value\"]\n",
    "    df_plot = pd.DataFrame(columns=header)\n",
    "\n",
    "    df_plot = pd.concat([df_pbf_unicycler[[\"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pbf_uni\").assign(score=\"prec\"), df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pbf_unicycler[[\"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"pbf_uni\").assign(score=\"rec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pbf_unicycler[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"pbf_uni\").assign(score=\"f1\"), \n",
    "        df_plot], ignore_index=True)\n",
    "\n",
    "    df_plot = pd.concat([df_pbf_skesa[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pbf_ske\").assign(score=\"prec\"), df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pbf_skesa[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"pbf_ske\").assign(score=\"rec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pbf_skesa[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"pbf_ske\").assign(score=\"f1\"), \n",
    "        df_plot], ignore_index=True)\n",
    "\n",
    "    df_plot = pd.concat([df_pbf_mean[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pbf-mean\").assign(score=\"prec\"), df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([df_pbf_mean[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"pbf-mean\").assign(score=\"rec\"), df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([df_pbf_mean[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"pbf-mean\").assign(score=\"f1\"), df_plot], ignore_index=True)\n",
    "\n",
    "\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_unicycler[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"gplas_uni\").assign(score=\"prec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_unicycler[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"gplas_uni\").assign(score=\"rec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_unicycler[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"gplas_uni\").assign(score=\"f1\"), \n",
    "        df_plot], ignore_index=True) \n",
    "\n",
    "\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_skesa[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"gplas_ske\").assign(score=\"prec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_skesa[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"gplas_ske\").assign(score=\"rec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_gplas_skesa[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"gplas_ske\").assign(score=\"f1\"), \n",
    "        df_plot], ignore_index=True) \n",
    "\n",
    "    df_plot = pd.concat([\n",
    "        df_pangebin[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pan\").assign(score=\"prec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pangebin[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"score_value\"}).assign(tool=\"pan\").assign(score=\"rec\"), \n",
    "        df_plot], ignore_index=True)\n",
    "    df_plot = pd.concat([\n",
    "        df_pangebin[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"score_value\"}).assign(tool=\"pan\").assign(score=\"f1\"),\n",
    "        df_plot], ignore_index=True)\n",
    "\n",
    "    # change header df plot\n",
    "    df_plot = df_plot.rename(columns={\"score_value\": \"value\"})\n",
    "    # change prec into Precision\n",
    "    df_plot[\"score\"] = df_plot[\"score\"].replace({\"prec\": \"Precision\", \"rec\": \"Recall\", \"f1\": \"F1-Score\"})\n",
    "\n",
    "    order = [\"gplas_ske\", \"gplas_uni\", \"pbf_ske\", \"pbf_uni\", \"pbf-mean\", \"pan\"]\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=6)\n",
    "    # palette = [\"blue\", \"green\", \"red\", \"yellow\", \"orange\", \"violet\"]\n",
    "    palette = [palette[0], palette[2], palette[1], palette[3], palette[5], palette[4]]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    p = sns.boxplot(\n",
    "        data=df_plot,\n",
    "        x=\"score\",\n",
    "        order=[\"Precision\", \"Recall\", \"F1-Score\"],\n",
    "        y=\"value\",\n",
    "        hue=\"tool\",\n",
    "        hue_order=order,\n",
    "        boxprops=dict(alpha=.5),\n",
    "        palette=palette,\n",
    "    )\n",
    "    # hide x axis label\n",
    "\n",
    "    sns.stripplot(data=df_plot, x=\"score\", y=\"value\", hue=\"tool\", hue_order=order,\n",
    "        dodge=True, ax=p, palette=palette, jitter=0.3).set(xlabel=None, ylabel=None)\n",
    "\n",
    "    handles, labels = p.get_legend_handles_labels()\n",
    "    p.legend(handles=[  (handles[0], handles[6]),\n",
    "                        (handles[1], handles[7]),\n",
    "                        (handles[2], handles[8]),\n",
    "                        (handles[3], handles[9]),\n",
    "                        (handles[4], handles[10]),\n",
    "                        (handles[5], handles[11])],\n",
    "            labels=['Gplas (S)', 'Gplas (U)', 'Plasbin-Flow (S)', 'Plasbin-Flow (U)', 'PlasBin-flow (mean)', 'Pangebin'],\n",
    "            loc='lower left', handlelength=6,\n",
    "            # handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "    )\n",
    "    sns.move_legend(p, \"lower center\", bbox_to_anchor=(0.5, -0.15), ncol=6, title=None, frameon=False)\n",
    "\n",
    "\n",
    "    # p.fig.subplots_adjust(top=.95)\n",
    "    plt.title(f\"{scoretype.upper()} for Gplas, Plasbin-Flow and Pangebin\")\n",
    "    # plt.xlabel(\"Plasbin-Flow\")\n",
    "    # plt.ylabel(\"Gplas\")\n",
    "    plt.savefig(f\"misc/plot/{scoretype}_whole_2.pdf\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_970399/3589639385.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_plot = pd.concat([df_pbf_unicycler[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pbf_uni\").assign(score=\"prec\"), df_plot], ignore_index=True)\n",
      "/tmp/ipykernel_970399/3589639385.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_plot = pd.concat([df_pbf_unicycler[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"score_value\"}).assign(tool=\"pbf_uni\").assign(score=\"prec\"), df_plot], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "boxplot(df_binning, \"binning\")\n",
    "boxplot(df_labeling, \"labeling\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute prec, rec, f1 mean for df_pangebin\n",
    "def print_scores(df):\n",
    "    prec = np.round(df[\"prec\"].mean(), 4)\n",
    "    rec = np.round(df[\"rec\"].mean(), 4)\n",
    "    f1 = np.round(df[\"f1\"].mean(), 4)\n",
    "    return float(prec), float(rec), float(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7185, 0.917, 0.7643)\n",
      "(0.6903, 0.9056, 0.7125)\n",
      "(0.5576, 0.9272, 0.6276)\n",
      "(0.8229, 0.884, 0.7973)\n",
      "(0.963, 0.8427, 0.8822)\n",
      "(0.9059, 0.6399, 0.7214)\n"
     ]
    }
   ],
   "source": [
    "load_scores(\"labeling\")\n",
    "print(print_scores(df_pangebin))\n",
    "print(print_scores(df_pbf_mean))\n",
    "print(print_scores(df_pbf_unicycler))\n",
    "print(print_scores(df_pbf_skesa))\n",
    "print(print_scores(df_gplas_unicycler))\n",
    "print(print_scores(df_gplas_skesa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6427, 0.8005, 0.6662)\n",
      "(0.6721, 0.7952, 0.6581)\n",
      "(0.5624, 0.8554, 0.6199)\n",
      "(0.7818, 0.7351, 0.6963)\n",
      "(0.8427, 0.7764, 0.7785)\n",
      "(0.8254, 0.5743, 0.6414)\n"
     ]
    }
   ],
   "source": [
    "load_scores(\"binning\")\n",
    "print(print_scores(df_pangebin))\n",
    "print(print_scores(df_pbf_mean))\n",
    "print(print_scores(df_pbf_unicycler))\n",
    "print(print_scores(df_pbf_skesa))\n",
    "print(print_scores(df_gplas_unicycler))\n",
    "print(print_scores(df_gplas_skesa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(df):\n",
    "    prec_m  = np.round(df[\"prec\"].mean(), 3)\n",
    "    rec_m = np.round(df[\"rec\"].mean(), 3)\n",
    "    f1_m = np.round(df[\"f1\"].mean(), 3)\n",
    "    prec_std = np.round(df[\"prec\"].std(), 3)\n",
    "    rec_std = np.round(df[\"rec\"].std(), 3)\n",
    "    f1_std = np.round(df[\"f1\"].std(), 3)\n",
    "    print(f\"${prec_m} \\\\pm {prec_std}$ \\t & ${rec_m} \\\\pm {rec_std}$ \\t & ${f1_m} \\\\pm {f1_std}$ \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.643 \\pm 0.289$ \t & $0.801 \\pm 0.194$ \t & $0.666 \\pm 0.235$ \\\\\n",
      "$0.672 \\pm 0.247$ \t & $0.795 \\pm 0.199$ \t & $0.658 \\pm 0.212$ \\\\\n",
      "$0.562 \\pm 0.302$ \t & $0.855 \\pm 0.184$ \t & $0.62 \\pm 0.245$ \\\\\n",
      "$0.782 \\pm 0.256$ \t & $0.735 \\pm 0.256$ \t & $0.696 \\pm 0.249$ \\\\\n",
      "$0.843 \\pm 0.209$ \t & $0.776 \\pm 0.195$ \t & $0.779 \\pm 0.186$ \\\\\n",
      "$0.825 \\pm 0.282$ \t & $0.574 \\pm 0.295$ \t & $0.641 \\pm 0.28$ \\\\\n"
     ]
    }
   ],
   "source": [
    "print_metrics(df_pangebin)\n",
    "print_metrics(df_pbf_mean)\n",
    "print_metrics(df_pbf_unicycler)\n",
    "print_metrics(df_pbf_skesa)\n",
    "print_metrics(df_gplas_unicycler)\n",
    "print_metrics(df_gplas_skesa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.718 \\pm 0.297$ \t & $0.917 \\pm 0.127$ \t & $0.764 \\pm 0.238$ \\\\\n",
      "$0.69 \\pm 0.265$ \t & $0.906 \\pm 0.165$ \t & $0.712 \\pm 0.224$ \\\\\n",
      "$0.558 \\pm 0.341$ \t & $0.927 \\pm 0.148$ \t & $0.628 \\pm 0.283$ \\\\\n",
      "$0.823 \\pm 0.269$ \t & $0.884 \\pm 0.202$ \t & $0.797 \\pm 0.25$ \\\\\n",
      "$0.963 \\pm 0.112$ \t & $0.843 \\pm 0.172$ \t & $0.882 \\pm 0.155$ \\\\\n",
      "$0.906 \\pm 0.266$ \t & $0.64 \\pm 0.283$ \t & $0.721 \\pm 0.278$ \\\\\n"
     ]
    }
   ],
   "source": [
    "load_scores(\"labeling\")\n",
    "print_metrics(df_pangebin)\n",
    "print_metrics(df_pbf_mean)\n",
    "print_metrics(df_pbf_unicycler)\n",
    "print_metrics(df_pbf_skesa)\n",
    "print_metrics(df_gplas_unicycler)\n",
    "print_metrics(df_gplas_skesa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trittico labeling\n",
    "\n",
    "# precision plasbin-flow pangebin blue\n",
    "# recall plasbin-flow pangebin verde\n",
    "# f1-score plasbin0flow pangebin arancio\n",
    "\n",
    "\n",
    "# df_pbf_skesa = df_labeling[(df_labeling[\"tool\"] == \"1.ske\") & (df_labeling[\"model\"] == \"pbf\")]\n",
    "# df_pbf_skesa_precision = df_pbf_skesa[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"skesa\"})\n",
    "\n",
    "# df_pbf_uni = df_labeling[(df_labeling[\"tool\"] == \"1.uni\") & (df_labeling[\"model\"] == \"pbf\")]\n",
    "# df_pbf_uni_precision = df_pbf_uni[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"uni\"})\n",
    "\n",
    "# df_pbf_precision = df_pbf_skesa_precision.merge(df_pbf_uni_precision, on=\"sample\")\n",
    "# df_pbf_precision.head()\n",
    "\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# p = sns.relplot(\n",
    "#     data=df_pbf_precision,\n",
    "#     y=\"skesa\",\n",
    "#     x=\"uni\",\n",
    "#     kind=\"scatter\",\n",
    "# ).set(xlim=(0, 1), ylim=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "def trittico(df_a, df_b, name_a, name_b, label, asm):\n",
    "    # assert(label in ['labeling', 'binning'])\n",
    "    \n",
    "    cblue = mcolors.TABLEAU_COLORS[\"tab:blue\"]\n",
    "    corange = mcolors.TABLEAU_COLORS[\"tab:orange\"]\n",
    "    cgreen = mcolors.TABLEAU_COLORS[\"tab:green\"]\n",
    "    cred = mcolors.TABLEAU_COLORS[\"tab:red\"] \n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.suptitle(f\"{label} scores for {name_a} (x axis) against {name_b} (y axis)\")\n",
    "\n",
    "\n",
    "    df_prec = df_a[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"A\"})\n",
    "    df_prec = df_prec.merge(df_b[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"B\"}),\n",
    "                              on=\"sample\")\n",
    "    df_rec = df_a[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"A\"})\n",
    "    df_rec = df_rec.merge(df_b[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"B\"}),\n",
    "                              on=\"sample\")\n",
    "    df_f1 = df_a[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"A\"})\n",
    "    df_f1 = df_f1.merge(df_b[[\"sample\", \"f1\"]].rename(columns={\"f1\": \"B\"}),\n",
    "                        on=\"sample\")\n",
    "    gs = fig.add_gridspec(1, 3, hspace=0, wspace=0.1)\n",
    "\n",
    "    (prec, rec, f1) = gs.subplots(sharex='col', sharey='row')\n",
    "    prec.scatter(x = df_prec[\"A\"], y=df_prec[\"B\"], color=cblue)\n",
    "    prec.axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "    prec.set_title(\"Precision\")\n",
    "\n",
    "\n",
    "    rec.scatter(x = df_rec[\"A\"], y=df_rec[\"B\"], color=corange)\n",
    "    rec.axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "    rec.set_title(\"Recall\")\n",
    "\n",
    "    f1.scatter(x = df_f1[\"A\"], y=df_f1[\"B\"], color=cgreen)\n",
    "    f1.axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "    f1.set_title(\"F1-Score\")\n",
    "\n",
    "    gs.update(left=0.07, right=0.97)\n",
    "    \n",
    "    fig.supxlabel(name_a)\n",
    "    fig.supylabel(name_b)\n",
    "    plt.savefig(f\"misc/plot/{label}.{asm}.scatter.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    # fig.xlabel(name_a)\n",
    "    # fig.ylabel(name_b)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_scores(\"labeling\")\n",
    "trittico(df_pangebin, df_pbf_mean, \"Pangebins\", \"Plasbin-Flow[mean]\", \"Labeling\", \"p\")\n",
    "load_scores(\"binning\")\n",
    "trittico(df_pangebin, df_pbf_mean, \"Pangebins\", \"Plasbin-Flow[mean]\", \"Binning\", \"p\")\n",
    "trittico(df_pangebin, df_pbf_unicycler, \"Pangebins\", \"Plasbin-Flow [unicycler]\", \"Binning\", \"u\")\n",
    "trittico(df_pangebin, df_pbf_skesa, \"Pangebins\", \"Plasbin-Flow [skesa]\", \"Binning\", \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doppietta(df_a, df_b, name_a, name_b, label, asm):\n",
    "    # assert(label in ['labeling', 'binning'])\n",
    "    \n",
    "    cblue = mcolors.TABLEAU_COLORS[\"tab:blue\"]\n",
    "    corange = mcolors.TABLEAU_COLORS[\"tab:orange\"]\n",
    "    cgreen = mcolors.TABLEAU_COLORS[\"tab:green\"]\n",
    "    cred = mcolors.TABLEAU_COLORS[\"tab:red\"] \n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    fig.suptitle(f\"{label} scores for {name_a} against {name_b}\")\n",
    "\n",
    "\n",
    "    df_prec = df_a[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"A\"})\n",
    "    df_prec = df_prec.merge(df_b[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"B\"}),\n",
    "                              on=\"sample\")\n",
    "    df_rec = df_a[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"A\"})\n",
    "    df_rec = df_rec.merge(df_b[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"B\"}),\n",
    "                              on=\"sample\")\n",
    "    \n",
    "    gs = fig.add_gridspec(1, 2, hspace=0, wspace=0.1)\n",
    "\n",
    "    (prec, rec) = gs.subplots(sharex='col', sharey='row')\n",
    "    prec.scatter(x = df_prec[\"A\"], y=df_prec[\"B\"], color=cblue)\n",
    "    prec.axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "    prec.set_title(\"Precision\")\n",
    "\n",
    "\n",
    "    rec.scatter(x = df_rec[\"A\"], y=df_rec[\"B\"], color=corange)\n",
    "    rec.axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "    rec.set_title(\"Recall\")\n",
    "\n",
    "    gs.update(left=0.09, right=0.97)\n",
    "    \n",
    "    fig.supxlabel(name_a)\n",
    "    fig.supylabel(name_b)\n",
    "    plt.savefig(f\"misc/plot/{label}.{asm}.scatter.dub.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    # fig.xlabel(name_a)\n",
    "    # fig.ylabel(name_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doppietta(df_pangebin, df_pbf_unicycler, \"Pangebins\", \"Plasbin-Flow (unicycler)\", \"Binning\", \"u\")\n",
    "doppietta(df_pangebin, df_pbf_skesa, \"Pangebins\", \"Plasbin-Flow (skesa)]\", \"Binning\", \"s\")\n",
    "doppietta(df_pangebin, df_pbf_mean,\"Pangebins\", \"Plasbin-Flow (mean)\", \"Binning\", \"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_6 \n",
    "load_scores(\"binning\")\n",
    "cblue = mcolors.TABLEAU_COLORS[\"tab:blue\"]\n",
    "corange = mcolors.TABLEAU_COLORS[\"tab:orange\"]\n",
    "cgreen = mcolors.TABLEAU_COLORS[\"tab:green\"]\n",
    "cred = mcolors.TABLEAU_COLORS[\"tab:red\"] \n",
    "\n",
    "fig = plt.figure(figsize=(13,17))\n",
    "fig.suptitle(f\"Binning scores for Pangebins against Plasbin-Flow\")\n",
    "\n",
    "\n",
    "# df_prec = df_a[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"A\"})\n",
    "# df_prec = df_prec.merge(df_b[[\"sample\", \"prec\"]].rename(columns={\"prec\": \"B\"}),\n",
    "#                             on=\"sample\")\n",
    "# df_rec = df_a[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"A\"})\n",
    "# df_rec = df_rec.merge(df_b[[\"sample\", \"rec\"]].rename(columns={\"rec\": \"B\"}),\n",
    "#                             on=\"sample\")\n",
    "\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.1, wspace=0.1)\n",
    "axss = gs.subplots(sharex='col', sharey='row')\n",
    "axss[0,0].scatter(x = df_pangebin[[\"prec\"]], y=df_pbf_skesa[[\"prec\"]], color=cblue)\n",
    "axss[0,0].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[0,0].set_title(\"Precision (skesa)\")\n",
    "\n",
    "axss[0,1].scatter(x = df_pangebin[[\"rec\"]], y=df_pbf_skesa[[\"rec\"]], color=corange)\n",
    "axss[0,1].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[0,1].set_title(\"Recall (skesa)\")\n",
    "\n",
    "axss[1,0].scatter(x = df_pangebin[[\"prec\"]], y=df_pbf_unicycler[[\"prec\"]], color=cblue)\n",
    "axss[1,0].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[1,0].set_title(\"Precision (unicycler)\")\n",
    "\n",
    "axss[1,1].scatter(x = df_pangebin[[\"rec\"]], y=df_pbf_unicycler[[\"rec\"]], color=corange)\n",
    "axss[1,1].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[1,1].set_title(\"Recall (unicycler)\")\n",
    "\n",
    "axss[2,0].scatter(x = df_pangebin[[\"prec\"]], y=df_pbf_mean[[ \"prec\"]], color=cblue)\n",
    "axss[2,0].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[2,0].set_title(\"Precision (skesa)\")\n",
    "\n",
    "axss[2,1].scatter(x = df_pangebin[[\"rec\"]], y=df_pbf_mean[[ \"rec\"]], color=corange)\n",
    "axss[2,1].axline( (0,0),slope=1,linestyle='--',color=cred)\n",
    "axss[2,1].set_title(\"Recall (mean)\")\n",
    "\n",
    "gs.update(left=0.09, right=0.97, top=0.93, bottom=0.05)\n",
    "\n",
    "\n",
    "fig.supxlabel(\"Pangebins\")\n",
    "fig.supylabel(\"Plasbin-Flow\")\n",
    "plt.savefig(f\"misc/plot/big.scatter.dub.pdf\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
